{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Benchmark of Ministry of Information Technologies and Communications \"100 Mil programadores\" program\n",
    "\n",
    "Data about the \"100 mil programadores\" program, each row is a students or an applicant.\n",
    "\n",
    "Details of the data can be readed in [Misión TIC 100 mil programadores](https://www.datos.gov.co/Ciencia-Tecnolog-a-e-Innovaci-n/Misi-n-TIC-2020-100-mil-programadores/2emd-i46m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the libraries to make the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import json\n",
    "import timeit\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup and download data\n",
    "\n",
    "Set \"download_data\" to True if you want download the data from [Datos Gov](www.datos.gov.co) page directly.\n",
    "\n",
    "The URL contain the query param to get all the data using the Socrata API (SODA API).\n",
    "Details about the use of that API can be viewed in:\n",
    "\n",
    "[SODA endpoints](https://dev.socrata.com/docs/endpoints.html)\n",
    "[SODA Pagination](https://dev.socrata.com/docs/paging.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_mintic2020_100mil = \"https://www.datos.gov.co/resource/2emd-i46m.json?$limit=135000\"\n",
    "\n",
    "response = requests.get(url = url_mintic2020_100mil)\n",
    "\n",
    "#TODO manage error\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define auxiliary functions to store and analyze data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(json_value, df_value, parquet_value):\n",
    "  results = {\n",
    "  \"json\" : json_value,\n",
    "  \"df\": df_value,\n",
    "  \"parquet\": parquet_value,\n",
    "  }\n",
    "  return results\n",
    "\n",
    "def analyze_dict(dict):\n",
    "  max_size = max(dict, key=dict.get)\n",
    "  min_size = min(dict, key=dict.get)\n",
    "  percentage= 100 - (times[min_size]*100/times[max_size])\n",
    "  return max_size, min_size, percentage\n",
    "\n",
    "def plot_dict(data, label_x, label_y, title):\n",
    "  keys = list(data.keys())\n",
    "  vals = [float(data[k]) for k in keys]\n",
    "  fig = sns.barplot(x=keys, y=vals)\n",
    "  fig.set(xlabel=label_x, ylabel=label_y, title=title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function declaration for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134563, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load df as \"colums\"\n",
    "df = pd.DataFrame.from_records(data)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data):\n",
    "    with open('./raw_data.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "def save_df(df):\n",
    "    df.to_pickle(\"./raw_data.pkl\")\n",
    "    \n",
    "def save_parquet(df):\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, './data_mintic.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.07 s ± 25.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit save_json(data)\n",
    "time_save_json = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742 ms ± 8.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit save_df(df)\n",
    "time_save_df = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239 ms ± 6.76 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit save_parquet(df)\n",
    "time_save_parquet = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = store_results(time_save_json.best,time_save_df.best,time_save_parquet)\n",
    "max_time, min_time, percentage_time = analyze_dict(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json():\n",
    "    with open('raw_data.json') as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.6 ns ± 0.595 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit read_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 ms ± 9.54 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pd.read_pickle(\"./raw_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.3 ms ± 172 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pq.read_table('./data_mintic.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare file sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the files are: 48.98 MB for json file,       18.57 MB for pkl file and       0.30 MB for parquet file.\n"
     ]
    }
   ],
   "source": [
    "size_json = os.path.getsize('./raw_data.json')\n",
    "size_pkl = os.path.getsize('./raw_data.pkl') \n",
    "size_parquet = os.path.getsize('./data_mintic.parquet')\n",
    "print(f'Size of the files are: {\"{:.2f}\".format(size_json/(1024*1024))} MB for json file, \\\n",
    "      {\"{:.2f}\".format(size_pkl/(1024*1024))} MB for pkl file and \\\n",
    "      {\"{:.2f}\".format(size_parquet/(1024*1024))} MB for parquet file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform operation over the data. The following aproach are taken:\n",
    "- Operate over the python list for json data (Search over the .json is not a practical aproach and will take more time)\n",
    "- Operate over the pandas dataframe for .pkl file.\n",
    "- Operate ever a pyarrow table for .parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = read_json()\n",
    "df_data = pd.read_pickle(\"./raw_data.pkl\")\n",
    "parquet_data = pq.read_table('./data_mintic.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 ns ± 2.06 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit parquet_data.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 ns ± 18.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit len(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.6 ns ± 1.91 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit len(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain a random element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.randint(0,len(df_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.7 µs ± 1.22 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit parquet_data.take([random_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.3 µs ± 2.98 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df_data.iloc[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.6 ns ± 1.95 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit json_data[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data to Mongo Atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup mongodb cluster, don forget add your DATABASE_URI as an environment variable according to your connection URI. See [mongo connect to your cluster](https://docs.atlas.mongodb.com/tutorial/connect-to-your-cluster/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(os.environ['DATABASE_URI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['platzimaster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mintic_collection = db['mintic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the data to the mongo cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_applicant(applicant_data):\n",
    "    \"\"\"\n",
    "    Return a boolean value of an insert operation to a mongodb collection\n",
    "    \n",
    "    Parameters:\n",
    "        applicant_data(dict): Dictionary to be stored as a BJSON in mongodb\n",
    "        \n",
    "    Returns:\n",
    "        True or False (bool) according to the result of the insert\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mintic_collection.insert_one(applicant_data)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"An exception occurred ::\", e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coping the data to send\n",
    "json_data_send = list(json_data)\n",
    "\n",
    "#Loop to send all the data\n",
    "while True:\n",
    "    #List to stor the boolean results during the send process\n",
    "    result = []\n",
    "    for register in json_data_send:\n",
    "        result.append(insert_applicant(register))\n",
    "    \n",
    "    unsended_registers = result.count(False)\n",
    "    if unsended_registers == 0:\n",
    "        print(\"All data sended\")\n",
    "        break\n",
    "        \n",
    "    print(f'Unsended registers: {unsended_registers}')\n",
    "    \n",
    "    json_data_send = [json_data_send[x] for x in result if x == False]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mintic_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
